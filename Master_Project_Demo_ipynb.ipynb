{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFssWlqn9ZtD"
      },
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfquA3-59cqR"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1iyDGGv9ct9"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3lwNqjM9cw1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gBu5qPt_Q4c"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c human-protein-atlas-image-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU2EL6BH_Q_s"
      },
      "outputs": [],
      "source": [
        "!unzip human-protein-atlas-image-classification.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CdVBZgzF_RCk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_dir = \"/content/data/\"\n",
        "print(\"File Name:\", os.listdir(data_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2TRy-qH_RFV"
      },
      "outputs": [],
      "source": [
        "# Check train\n",
        "print(\"Train:\", len(os.listdir(os.path.join(data_dir, \"train\"))))\n",
        "\n",
        "# Check test\n",
        "print(\"Test:\", len(os.listdir(os.path.join(data_dir, \"test\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U2nEa3K_RLs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Train CSV\n",
        "train_csv_path = os.path.join(data_dir, \"train.csv\")\n",
        "train_labels = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Check\n",
        "print(train_labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNTUByH_Blcd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "NUM_CLASSES = 28\n",
        "\n",
        "# One-hot\n",
        "def multi_label_encoding(labels):\n",
        "    encoded = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
        "    for label in labels.split():\n",
        "        encoded[int(label)] = 1\n",
        "    return encoded\n",
        "\n",
        "train_labels['Encoded_Labels'] = train_labels['Target'].apply(multi_label_encoding)\n",
        "\n",
        "print(train_labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb01nQs7BljG"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# define picture preprocessing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # same size\n",
        "    transforms.RandomHorizontalFlip(),  # random flipping\n",
        "    transforms.RandomRotation(15),  # rotat\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),  # Convert to tensors\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # normalization\n",
        "])\n",
        "\n",
        "data_dir = \"/content/data/\"\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "\n",
        "# Get a list of all image files in the train directory\n",
        "image_files = [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]\n",
        "\n",
        "# Check if there are any image files in the directory\n",
        "if image_files:\n",
        "    # Use the first image file in the list\n",
        "    img_path = os.path.join(train_dir, image_files[0])\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    image = train_transform(image)\n",
        "    print(image.shape)\n",
        "else:\n",
        "    print(\"No image files found in the train directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "C_NHX9PeMOYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training data\n",
        "data_dir = \"/content/data\"\n",
        "train_csv_path = os.path.join(data_dir, \"train.csv\")\n",
        "train_images_path = os.path.join(data_dir, \"train\")\n",
        "\n",
        "# read CSV\n",
        "train_labels = pd.read_csv(train_csv_path)\n",
        "print(\"training data:\", len(train_labels))\n",
        "print(\"first lines:\\n\", train_labels.head())\n",
        "\n",
        "# Target string type\n",
        "train_labels[\"Target\"] = train_labels[\"Target\"].astype(str)"
      ],
      "metadata": {
        "id": "xwzKhPZFMRV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a ProteinDataset"
      ],
      "metadata": {
        "id": "ZiLSVXCbNuLZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpvi3uGDHnbG"
      },
      "outputs": [],
      "source": [
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.dataframe.iloc[idx][\"Id\"]\n",
        "        labels = self.dataframe.iloc[idx][\"Target\"]\n",
        "\n",
        "        # read R/G/B/Y images\n",
        "        img_red = Image.open(os.path.join(self.img_dir, f\"{img_id}_red.png\"))\n",
        "        img_green = Image.open(os.path.join(self.img_dir, f\"{img_id}_green.png\"))\n",
        "        img_blue = Image.open(os.path.join(self.img_dir, f\"{img_id}_blue.png\"))\n",
        "        img_yellow = Image.open(os.path.join(self.img_dir, f\"{img_id}_yellow.png\")).convert(\"L\")\n",
        "\n",
        "        # Convert to tensors\n",
        "        img_red = transforms.ToTensor()(img_red)\n",
        "        img_green = transforms.ToTensor()(img_green)\n",
        "        img_blue = transforms.ToTensor()(img_blue)\n",
        "        img_yellow = transforms.ToTensor()(img_yellow)\n",
        "\n",
        "        # Merge into 4 channels\n",
        "        image = torch.cat([img_red, img_green, img_blue, img_yellow], dim=0)\n",
        "\n",
        "        # transfer Target to One-hot encoding\n",
        "        NUM_CLASSES = 28\n",
        "        encoded_labels = torch.zeros(NUM_CLASSES, dtype=torch.float32)\n",
        "        for label in str(labels).split():\n",
        "            encoded_labels[int(label)] = 1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, encoded_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing & training only 1/3 of the data"
      ],
      "metadata": {
        "id": "OWits2R_Nq19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Data Transformations (Normalize & Resize)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # transforms.ToTensor(),  # Remove this line as it's already done in __getitem__\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Create a dataset\n",
        "train_dataset = ProteinDataset(train_labels, train_images_path, transform=train_transform)\n",
        "\n",
        "# Pick 1/3 of the data index\n",
        "subset_size = len(train_dataset) // 3\n",
        "subset_indices = np.random.choice(len(train_dataset), size=subset_size, replace=False)\n",
        "\n",
        "# Use 'SubsetRandomSampler' to train only 1/3 of the data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, sampler=SubsetRandomSampler(subset_indices))\n",
        "\n",
        "# Test DataLoader\n",
        "for images, labels in train_loader:\n",
        "    print(\"Batch image shape:\", images.shape)\n",
        "    print(\"Batch label sahpe:\", labels.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "O6hnctsJM7g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining CNNs (DenseNet121, supports 4-channel inputs)"
      ],
      "metadata": {
        "id": "Agfl1-q0NjoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DenseNet121 train the model\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer to support 4-channel inputs\n",
        "model.features.conv0 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modifying the Last Layer Classifier (Class 28)\n",
        "num_ftrs = model.classifier.in_features\n",
        "model.classifier = nn.Linear(num_ftrs, 28)\n",
        "\n",
        "# Send to GPU (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "GUtXS0rtNNJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the CNN\n",
        "other loss function, BCEloss-different weights,\n"
      ],
      "metadata": {
        "id": "xmz2O6DZN4lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Successful！\")"
      ],
      "metadata": {
        "id": "a8rfSCrjNfO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n",
        "confusion matrix"
      ],
      "metadata": {
        "id": "flA6-Z5JN_y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "evaluate_model(model, train_loader)"
      ],
      "metadata": {
        "id": "bwDU5TbhOAkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict new pictures"
      ],
      "metadata": {
        "id": "BmHyHlDLOD8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, img_id):\n",
        "    model.eval()\n",
        "\n",
        "    # Update train_images_path\n",
        "    train_images_path = \"/content/data/train\"\n",
        "\n",
        "    # read R/G/B/Y images\n",
        "    img_red = Image.open(os.path.join(train_images_path, f\"{img_id}_red.png\"))\n",
        "    img_green = Image.open(os.path.join(train_images_path, f\"{img_id}_green.png\"))\n",
        "    img_blue = Image.open(os.path.join(train_images_path, f\"{img_id}_blue.png\"))\n",
        "    img_yellow = Image.open(os.path.join(train_images_path, f\"{img_id}_yellow.png\")).convert(\"L\")\n",
        "\n",
        "    # Convert to Tensor and splic\n",
        "    img_red = transforms.ToTensor()(img_red)\n",
        "    img_green = transforms.ToTensor()(img_green)\n",
        "    img_blue = transforms.ToTensor()(img_blue)\n",
        "    img_yellow = transforms.ToTensor()(img_yellow)\n",
        "\n",
        "    image = torch.cat([img_red, img_green, img_blue, img_yellow], dim=0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        output = torch.sigmoid(output).cpu().numpy()\n",
        "\n",
        "    predicted_labels = (output[0] > 0.5).astype(int)\n",
        "    return predicted_labels\n",
        "\n",
        "# Prediction example\n",
        "sample_img_id = \"0007d0f0-bbc3-11e8-b2bc-ac1f6b6435d0\"\n",
        "predicted_labels = predict(model, sample_img_id)\n",
        "print(f\"Predicted Labels: {predicted_labels}\")"
      ],
      "metadata": {
        "id": "l4vVL7g1OE4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}